[{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Video Source\nPage Source\nPratical Source\nQuestions?\n1. Introduction Extension of Abstract. Sequence Modeling: Recurrent language models\nOperate step-by-step, processing one token or time step at a time. one input + hidden state → one output + hidden state (update_every_time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or_other_models): one to encode input into a representation another to decode it into the output sequence one input + hidden state → … → hidden state → one output + hidden state → … e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it’s not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn’t use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background **Corresponding Work:** Speak clearly about the corresponding papers, the connections between u and the papers, and the differences. Reduce sequential computation: Typically use convolutional neural networks → the number of operations grows in the distance between positions → transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ✔ Reccurrence attention mechanism\n❌ Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model.\n(to_compute_representations_of_its_input_and_output_without_using_sequence-aligned_RNNsorconvolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x → a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n(Given_a_query,_output_is_computed_by_similarity_between_query_and_keys,_then_different_keys_have_different_weights,_next_combine_the_values_based_on_the_weights.)\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query √, and apply a softmax function to obtain the weights on the values.\n**Why divide each by length of query √ ?** It prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026#39;t address the problem of **large input magnitudes** directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation. n refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\n**What there is a function of the Mask?** When computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs. 3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn “encoder-decoder attention” layers, queries → previous decoder layer keys and values → the output of the encoder In “encoder self-attention” layers, keys, values and queries all come from the previous layer in the encoder In “decoder self-attention” layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and **MLP patterns** are the same. The difference are the **input source**. 3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension (512), then multiply those weights by\n$\\sqrt {d_{models}}$\n**Why multiply those weights by $\\sqrt {d_{models}}$ ?** To **boosts the magnitude**, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution. We update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction. Softmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don’t change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word → d\nNumber of words → n\nSelf-Attention → **n words *** every words need to multiply with n words and for ****each two words do d multiplying.\nRecurrent → d-dimension vector multiply d*d matrix, n times\nConvolutional → k kernel_size, n words, d^2 input_channels * output_channels (Draw_picture_clear)\nSelf-Attention (restricted) → r the number of neighbors\nIt seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training **Experiment:** 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. → So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"http://localhost:1313/posts/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"Questions-178559a021ef8081a15fe31b403a1d64?pvs=21\"\u003eQuestions?\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eExtension of Abstract.\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language models\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → one output + hidden state (update_every_time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or_other_models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → … → hidden state → one output + hidden state → …\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it’s not good.\u003c/p\u003e","title":"Attention is All You Need"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state → one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state → … → hidden state → one output + hidden state → … e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it’s not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn’t use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks → the number of operations grows in the distance between positions → transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ✔ Reccurrence attention mechanism\n❌ Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x → a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query √, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query √ ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn “encoder-decoder attention” layers, queries → previous decoder layer keys and values → the output of the encoder In “encoder self-attention” layers, keys, values and queries all come from the previous layer in the encoder In “decoder self-attention” layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don’t change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word → d Number of words → n Self-Attention → **n words *** every words need to multiply with n words and for ****each two words do d multiplying. Recurrent → d-dimension vector multiply d*d matrix, n times Convolutional → k kernel_size, n words, d^2 input_channels * output_channels (Draw picture clear) Self-Attention (restricted) → r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. → So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"http://localhost:1313/posts/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → … → hidden state → one output + hidden state → …\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it’s not good.\u003c/p\u003e","title":"Attention is All You Need"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state → one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state → … → hidden state → one output + hidden state → … e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it’s not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn’t use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks → the number of operations grows in the distance between positions → transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ✔ Reccurrence attention mechanism\n❌ Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x → a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query √, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query √ ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn “encoder-decoder attention” layers, queries → previous decoder layer keys and values → the output of the encoder In “encoder self-attention” layers, keys, values and queries all come from the previous layer in the encoder In “decoder self-attention” layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don’t change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word → d Number of words → n Self-Attention → n words ✖️ every words need to multiply with n words and for ****each two words do d multiplying. Recurrent → d-dimension vector multiply d✖️d matrix, n times Convolutional → k kernel_size, n words, d^2 input_channels ✖️ output_channels (Draw picture clear) Self-Attention (restricted) → r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. → So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"http://localhost:1313/posts/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → … → hidden state → one output + hidden state → …\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it’s not good.\u003c/p\u003e","title":"Attention is All You Need"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"http://localhost:1313/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"http://localhost:1313/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"}]