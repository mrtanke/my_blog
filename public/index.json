[{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn‚Äôt need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It‚Äôs like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better ‚Äî for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"https://my-blog-alpha-vert.vercel.app/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths ‚Üí impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it‚Äôs hard to know who‚Äôs winning Humans can‚Äôt easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo üí° Imagine AlphaGo is a smart player who has:\nintuition ‚Üí from the policy network judgment ‚Üí from the value network planning ability ‚Üí from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves ‚Üí the probability of each move. Value Network: Evaluates board positions ‚Üí the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. üí° MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) ‚Üí the probability of the move from policy network Visit Count (N) ‚Üí how many times we‚Äôve already explored this move during simulations. Q-Value (Q) ‚Üí average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: ‚Äúkeep doing what worked‚Äù Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5‚Äì0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ‚âà $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ‚âà 250, $d$ ‚âà 150) ‚Üí bigggg number ‚Üí impossible to compute exhaustively. Reducing the search space ‚Äî two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (‚Äúintractable‚Äù) for Go because Go‚Äôs positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. üí° ‚ÄúSimulate‚Äù and ‚ÄúRoll out‚Äù basically mean the same thing in this context.\n‚ÄúSimulate‚Äù ‚Üí a general word: to play out an imaginary game in your head or computer. ‚ÄúRoll out‚Äù ‚Üí a more specific term from Monte Carlo methods, meaning ‚Äúplay random moves from the current position until the game ends.‚Äù So ‚Üí every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves ‚Üí reduce breadth). Recording each game‚Äôs result (+1 for win, ‚àí1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt‚Äôs simple but inefficient ‚Äî great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go‚Äôs search space was too large. Training pipeline of AlphaGo üí° Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network ‚Üí replaces long rollouts (reduces search depth). Sampling moves with a policy network ‚Üí focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves ‚Üí stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_œÉ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks üí° $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 ¬µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights œÅ = œÉ Step What happens What‚Äôs learned Initialize Copy weights from SL policy (œÅ = œÉ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, ‚àí1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights œÅ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What‚Äôs learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / ‚àí1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_Œ∏(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pœÄ, pœÅ) Matches rollout accuracy with 15 000√ó less computation Result MSE ‚âà 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS üí° Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation ‚Üí overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ‚âà 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) ![image.png](./image 1.png)\nPanel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ‚ë† Value network (v_Œ∏(s)): predicts win probability instantly. ‚ë° Rollout with fast policy p_œÄ: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_Œ∏(s)) and (r)) back up the tree ‚Äî update each parent node‚Äôs Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability ‚Äî how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection ‚Äî choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\nüí° $Q(s,a)$: ‚ÄúHow good this move has proven so far.‚Äù $u(s,a)$: ‚ÄúHow much we should still explore this move.‚Äù ‚Üí Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks ‚Äî the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown ‚Äî this new node represents a new possible future board.\nStep 3: Evaluation ‚Äî estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_Œ∏(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_œÄ$) until the game ends +1 if win ‚àí1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - Œª)v_Œ∏(s_L) + Œªz_L $$\n$Œª$ = mixing parameter (balances between value net and rollout). If $Œª$ = 0.5, both count equally. Step 4: Backup ‚Äî update the tree statistics The leaf‚Äôs evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\nüí° $1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation‚Äôs leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vŒ∏$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) ‚Äî the most explored and trusted move.\nüí° Why SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts ‚Üí gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning ‚Üí focuses too narrowly on top 1‚Äì2 moves MCTS loses diversity ‚Üí gets less exploration benefit So, for MCTS‚Äôs exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior ‚Äî because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nüí° All programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. üí° Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. üí° Policy network ‚Üí ‚Äúprobability of choosing a move‚Äù\nValue network ‚Üí ‚Äúprobability of winning from a position‚Äù\n","permalink":"https://my-blog-alpha-vert.vercel.app/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths ‚Üí impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it‚Äôs hard to know who‚Äôs winning\u003c/td\u003e\n          \u003ctd\u003eHumans can‚Äôt easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\nüí°\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"}]